{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Artificial Intelligence\n",
    "\n",
    "[link](https://classroom.udacity.com/courses/cs271)\n",
    "\n",
    "### Intelligent Agents\n",
    "\n",
    "An AI program is called an Intelligent Agent\n",
    "\n",
    "![](./data/img/diag1.png)\n",
    "\n",
    "There is an IA and there is an environment. The agent gets to interact with the env\n",
    "\n",
    "It can perceive the state of the env thru sensors\n",
    "\n",
    "It can affect its state thru its actuators\n",
    "\n",
    "But there needs to be a function that maps sensors to actuators. This is the key\n",
    "\n",
    "**How does an agent make decisions that it can carry out with its actuautors based on past sensor data**\n",
    "\n",
    "Those decisions take place many, many times in a loop : environment feedback -> sensor -> agent decision -> actuators interation with env and so on.. This is called **Perception Action Cycle**\n",
    "\n",
    "\n",
    "### Basic Terminologies in AI\n",
    "\n",
    "1. Fully vs Partially observable\n",
    "\n",
    "An env is called fully observable if what the agent can sense at any pt of time is **completely sufficient** to make the optimal decision\n",
    "\n",
    "For eg, in a card game when all the cards are on the table, the sight of all the cards is sufficient\n",
    "\n",
    "But in poker u need memory of previous card moves\n",
    "\n",
    "![](./data/img/diag2.png)\n",
    "\n",
    "For many envs it is convenient to assume that the env has some internal state. For eg, in the card game where the state may be \"cards in the hand\". An env is **fully observable** if the sensors can always see the entire state of the env\n",
    "\n",
    "It is partially obs if the sensors can only see a fraction of the state yet measuring past info give us addntl info of the state that is not readily observable right now. So any game where past moves might be an indication of cards in a person's hand.. those games are PO and require differ treatment. Here agents would require internal memory\n",
    "\n",
    "2. Deterministic vs Stochastic\n",
    "\n",
    "Deterministic env is one where the agent's actions uniquely determine the outcome. \n",
    "In chess, the effect of moving a piece is completely predetermined\n",
    "\n",
    "Stochastic: Outcome of an action involves throwing of a dice, there is a certain randomness involved.\n",
    "Poker is also Stochastic because u are dealt cards, which is random\n",
    "\n",
    "3. Discrete vs Cont\n",
    "\n",
    "Discrete : finite action choices and finite many things u can sense. In chess, finite board positions and finite no of things u can do\n",
    "\n",
    "Cont: space of possible actions or things u can sense may be infiinite\n",
    "Throwing darts: infinite angles\n",
    "\n",
    "4. Benign vs Adversarial env\n",
    "\n",
    "Benign: env might be stochastic but it has no objective on its own that would contradict ur own obj\n",
    "Whether is benign.. \n",
    "Adversarial : chess..opponent is trying to beat u\n",
    "\n",
    "### Quiz:\n",
    "\n",
    "![](./data/img/diag3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI in Uncertainity Management\n",
    "\n",
    "![](./data/img/diag4.png)\n",
    "\n",
    "Some of the reasons for uncertainity are:\n",
    "\n",
    "1. Sensor limits\n",
    "2. Adversaries (opponent trying to beat u in chess)\n",
    "3. Stochastic Env (randomness involved like throwing of dice)\n",
    "4. Laziness (program is too lazy)\n",
    "5. Ignorance\n",
    "\n",
    "### Examples in AI\n",
    "\n",
    "1. Language Translation by Google\n",
    "\n",
    "How it workks is we have two texts, in 2 diff languages and we try and find a correspondence between them\n",
    "\n",
    "![](./data/img/diag5.png)\n",
    "\n",
    "\n",
    "![](./data/img/diag6.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit 2 - Problem Solving\n",
    "\n",
    "In problem solving, like in navigation, there are many states\n",
    "\n",
    "![](./data/img/diag7.png)\n",
    "\n",
    "Here there are many choices to start with and many choices at each subsequent step\n",
    "\n",
    "![](./data/img/diag8.png)\n",
    "\n",
    "Here complexity is because of the partial observability of the sys\n",
    "\n",
    "The consequences of the actions, and evne the actions themselves are not known.\n",
    "\n",
    "We will focus on the first part as of now\n",
    "\n",
    "If we are given a map and we are asked to find a route bw Arad and Bucharest aand if Bucharest is not on the map, teh agent will not be able to find a soln. This is because the agent does not know of any **action** that can go there\n",
    "\n",
    "![](./data/img/diag9.png)\n",
    "\n",
    "\n",
    "**Defn of a Problem**:\n",
    "\n",
    "A problem is defined by:\n",
    "\n",
    "- Initial State: Here it is Arad\n",
    "\n",
    "- Actions(State) returns set of possible actions that agent can execute when it is in that state\n",
    "    \n",
    "    ACTION(S) -> {a1, a2, a3...}\n",
    "    In some problems the agent will have same action on all states and in other probs they will have diff actions dependent on the state. In this prob the action is dependent on the state. In a particular city, we can take the routes to the neighboring cities\n",
    "    \n",
    "- RESULT(S, a) -> S1\n",
    "    This takes a state and action as input and returns a new state\n",
    "    If agent is in Arad (state) and action is driving to Timesora. The op is the state of Timesora\n",
    "    \n",
    "- GOALTEST(S) -> True/False\n",
    "    GOALTEST(S=Bucharest) = True\n",
    "    \n",
    "- PATHCOST(takes a sequence of State action transitions) -> returns a number which is the cost of that path\n",
    "\n",
    "    For most probs we make the PATHCOST func additive. So the cost of the path is just the sum of the costs of indv states. So we will implement the PATHCOST func as a STEPCOST func\n",
    "    STEPCOST(S, a, S1) -> n\n",
    "    It takes a State, action and resulting state from that actiona nd returns the cost of that action. In the eg, cost might be no of miles or time\n",
    "\n",
    "    \n",
    "    \n",
    "**State Space** : Set of all the states. We navigate the state space by applying actions\n",
    "\n",
    "The actions are specific to each city. When in Arad there are 3 actions, to Z, S or T\n",
    "As we follow them we build path, or sequences of actions\n",
    "\n",
    "From A, we ge to Z, S or T (paths of length 1)\n",
    "\n",
    "S -> R or F | Z -> O | T -> L\n",
    "\n",
    "At every step, we want to separate the steps out into 3 parts\n",
    "\n",
    "![](./data/img/diag10.png)\n",
    "\n",
    "\n",
    "1. The ends of the paths, the farthest paths that have been explored - Frontier\n",
    "    This is shown in green\n",
    "    The Frontier consists of the states - L,P,FO\n",
    "    \n",
    "2. We have the explored states\n",
    "\n",
    "3. We have the unexplored states\n",
    "\n",
    "![](./data/img/diag11.png)\n",
    "\n",
    "\n",
    "Also in this diag the step costs have been written and the path cost is simply the sum of the step costs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
